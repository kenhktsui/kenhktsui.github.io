<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IrlnTHVAWoMVTuk5KL-kjg.png"></figure> <p><strong>Motivation:</strong></p> <p>Next token prediction imposes a constraint that the no. of operation predicting the next token is limited by the no. of token seen so far.</p> <p><strong>Hypothesis:</strong></p> <p>For some task, it demands more computation than there is, in next token prediction. By introducing &lt;pause&gt; token, it induces more computation, and therefor improve performance.</p> <p><strong>Reasoning Type:</strong></p> <p>Deductive</p> <p><strong>Reasoning Step:</strong></p> <p>To explain what it means, I drew the below to illustrate the effect of delaying next token prediction. The graph on the left is the standard next token prediction, the next token prediction goes over 2 transformer blocks to predict “jumps”.</p> <p>The graph on the right is after injecting the &lt;pause&gt; token. Due to the autoregressive nature of the model where the output in the previous step fed back as an input for the next step, the prediction goes through 4 transformers blocks before the model output “jumps”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/993/1*7JAEK3yAmxBZXxj99VARAA.png"></figure> <p>As such, by introducing &lt;pause&gt; token, it induces more computation without changing model size.</p> <p><strong>Testing Approach:</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8j8qbDW_jjhuUnHMEaK0WQ.png"><figcaption>Figure 1 from the Paper</figcaption></figure> <p>Two objectives were tested:</p> <ul> <li>does delay token prediction improvements improvement?</li> <li>when shall &lt;pause&gt; token be introduced, in pretraining or finetuning or both?</li> </ul> <p>Decoder model of 1B and 130M was pretrained and finetuned, (probably due to computation resources because pretraining is required). Please also not that the &lt;pause&gt; token does not contribute into loss being optimised.</p> <p>Nine datasets were used.</p> <ul> <li>GSM8k — high quality grade school maths</li> <li>SQuAD V1 —question answering based on Wikipedia</li> <li>CommonSenseQA — questions on common sense</li> <li>LAMBADA — last word prediction on long context</li> <li>WebQA — open Domain questions with multi-hop nature</li> <li>NaturalQA — open Domain questions based on Wikipedia</li> <li>CoQA — question answering based on context</li> <li>PhysicalIQA — questions on physical common sense</li> <li>HellaSwag — commonsense natural language inference</li> </ul> <p><strong>Findings:</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Da91W17JLQp1BbVARfQ57Q.png"></figure> <p>Introducing &lt;pause&gt; tokens in pretraining and finetuning outperforms standard pretraining and finetuning in most QA tasks except HellaSwag. Finetuning with &lt;pause&gt; tokens after standard pretraining has mixed result.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wr7rTDofKPvOzhhmunC-Hw.png"></figure> <p>Adding filler token during inference time in standard pretrained and finetuned does not help performance.</p> <p>For each dataset, there exists an optimal no of &lt;pause&gt;.</p> <p><strong>My Take:</strong></p> <p>Despite limitations such as limited testing in bigger model sizes and benchmarking performance with larger model, I personally like this paper very much because it revisits the next token prediction objective, which is quite under-researched relatively in my opinion. The authors have opened the paradigm of delayed token prediction, and challenged the implicit assumption that the no. of operation predicting the next token is limited by the no. of token seen.</p> <p>Also, the author also laid out a very important open question: model parameter count vs computation pathways.</p> <p>In short, the idea is simple, but it challenged the existing assumption (next token prediction) and as such open the doors to many area to researchers.</p> <p>Inspired by the paper, I am extending my thoughts further:</p> <h4>Required Computation Complexity is Different For Each Token</h4> <p>Not every token prediction requires the same amount of computation.</p> <p>Sentence 1: [Description of Case] . The culprit ___</p> <p>Sentence 2:[Description of Case]. The culprit is ___</p> <p>For example, prediction in sentence 1 because it’s bounded by grammatical rule while prediction in sentence 2 requires very long reasoning (amount of computation) because guessing a culprit requires hypothesis, search, and elimination of hypothesis.</p> <p>Perhaps this is not very surprising. My <a href="https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231" rel="external nofollow noopener" target="_blank">previous paper review</a> highlights hallucination snowballing. One of the causes its authors attributed into is “Inherently sequential” - transformer cannot find an answer within one time step because of limiting reasoning ability in limited tokens. Therefore, a LM cannot answer a question requiring multiple steps of reasoning if a LM is guided to answer in one step.</p> <p>This is the nature of text. There is no correlation between the no of words and the amount of thought required to produce the text, because some thoughts were documented while most thoughts were not.</p> <p>This also explains why different dataset has an optimal no. of &lt;pause&gt; for the best performance. Some datasets are just harder than others, and requires more reasoning and computation.</p> <p>Future direction can be to leverage uncertainty of a language model prediction to construct a dataset that is filled with no of &lt;pause&gt; that corresponds to uncertainty.</p> <h4>Prompt Engineering As A Rescue?</h4> <p>Prompt engineering like <a href="https://arxiv.org/pdf/2201.11903.pdf" rel="external nofollow noopener" target="_blank">Chain of Thought</a>, few shot learning, or simply to append “Let’s think step by step” despite different intentions share one commonalities, which is to increase the amount of computation. For example, chain of thought prompts model to generate intermediate reasoning step before producing answer, as a rescue to provide more reasoning step. Few shot learning requires seeing a few demonstration before the prediction.</p> <p>Prompt engineering has a limitation that it can only be done in inference time, and sometimes there could be a mismatch between pretraining objective and inference setting. There can be an attempt to pretrain a model with a dataset with COT prompting, but it’s just so difficult to ensure the whole web text is filled with reasoning.</p> <h4>Soft Token vs Hard Token</h4> <p>Another question is if reasoning step has to be in language form? To a language model, token possesses much less information than vector. In chain of thought, one could always challenge why this reasoning step is used but not others? In fact, towards the same solution, there are many possible ways of reasoning. With language, we usually go with one way, but not all the ways. It is why language model succeeded because languaged is modeled as probabilities. And probabilities means there are different ways to express to same meaning.</p> <p>I have a conjecture that &lt;pause&gt; token serves as the soft version of chain of thought. Indeed soft token is not something new. Soft prompt was proved to boost performance just like hard prompt. <a href="https://arxiv.org/pdf/2104.08691.pdf" rel="external nofollow noopener" target="_blank">Prompt Tuning</a> is one of the examples, which prepends a series of tokens to condition model generation for specific tasks. The token is stored in embedding table.</p> <h4>Allocation of FLOP budget (Width vs Depth vs Model Size)</h4> <p>Computation pathway is an under discussed topic in the research field. There were research on how to achieve compute-optimal training by the famous <a href="https://arxiv.org/pdf/2203.15556.pdf" rel="external nofollow noopener" target="_blank">Chinchilla</a> paper, which answers the question of how to allocate model size and no. of training token given the same compute during training.</p> <p>With the model size and computation pathways being orthogonal, some open questions are:</p> <ul> <li>What is the optimal balance of model parameter count vs computation pathways?</li> <li>Can a 7B model perform similarly to a 175B model by increasing computation depth L to get roughly equal number of floating point operations?</li> <li>How will this affect the design of transformers going forward? Can we reuses transformer block over time step, like parameter sharing across time step in RNN?</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*az1EAfFSFZsYbXTIk9KNzg.png"><figcaption>Transformer Block 1 and 2 are reused.</figcaption></figure> <ul><li>Given the same parameter count, shall we prioritise less on no. of transformers block, while re-allocate more on dimension?</li></ul> <p>These are all interesting topics that I might conduct research and write a formal paper in the future if I have time.</p> <p><strong>About Paper Digest:</strong></p> <p>Paper Digest aims to digest a paper into a short summary while maintaining the essence of the scientific process behind the research. It serves as my personal reflection after understanding an academic paper.</p> <p>Previous articles:</p> <p><a href="https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231" rel="external nofollow noopener" target="_blank">Paper Digest: How Language Model Hallucinations Can Snowball</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=42be8c1ae785" width="1" height="1" alt=""></p> </body></html>