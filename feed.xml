<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kenhktsui.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kenhktsui.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-20T12:55:50+00:00</updated><id>https://kenhktsui.github.io/feed.xml</id><title type="html">Ken Tsui</title><subtitle>Ken Tsui - Deep Learning Engineer specializing in machine learning, large language models, and multimodal learning. Explore my projects, blogs and and publications. </subtitle><entry><title type="html">Embodied AI == Unlimited Training Data</title><link href="https://kenhktsui.github.io/blog/2025/embodied-ai-unlimited-training-data/" rel="alternate" type="text/html" title="Embodied AI== Unlimited Training Data"/><published>2025-01-13T14:25:25+00:00</published><updated>2025-01-13T14:25:25+00:00</updated><id>https://kenhktsui.github.io/blog/2025/embodied-ai--unlimited-training-data</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2025/embodied-ai-unlimited-training-data/"><![CDATA[<h3>The End of Internet Data Scarcity: Why Embodied AI Changes Everything</h3> <p>We’re at a pivotal moment in AI history. While industry leaders declare the end of pre-training as we know it, there is a missing crucial insight: we’re not facing the end of pre-training-the next frontier isn’t about scraping more data from the internet; it’s about tapping into the boundless stream of real-world data through embodied AI.</p> <p>Consider this: the entire English internet’s training data, accumulated over decades, equals just 15.6 years of footage from a single camera. Now imagine a million cameras, each capturing the world 24/7. This isn’t just an incremental change in how we collect data-it’s a paradigm shift that could fundamentally transform how AI learns and understands our world.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/774/0*EilzdwsvX5yDwCvQ.png"/><figcaption><em>Ilya Sutskever’s Presentation @ NeurIPS 2024</em></figcaption></figure> <h3>The Death of Pre-training? Not Quite</h3> <p>In NeurIPS 2024, Ilya Sutskever said pre-training will end. But let’s dig deeper: what’s really ending is our reliance on internet-sourced data. Why? Because every piece of internet content-every article, poem, textbook, and artwork-requires human effort to create and curate. We’re hitting the ceiling of human content creation capacity. But what if we stopped depending on human-created data entirely?</p> <h3>The Hidden Economics of Data Collection</h3> <p>Numbers tell this story better than words ever could. Let me show you something striking: while it takes months to generate 1M training tokens from written content, the same amount of data flows through just 32.8 seconds of real-world video capture. But here’s the critical nuance: I’m not suggesting that a text token and a video token are equivalent-they capture fundamentally different aspects of information. A text token might encode abstract concepts and relationships, while a video token captures visual patterns, motion, and physical interactions.</p> <p>The real revelation isn’t about token equivalence-it’s about scale. Even accounting for these differences in information density, the sheer velocity of real-world data collection is staggering. Think about this: while you’re reading this article, a network of 1M cameras could generate 1T training tokens. For perspective, <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">FineWeb</a>, the largest open-source English training dataset, contains just 15T tokens-equivalent to 15.6 years of a single camera’s capture.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/0*s8I8B9PP3lmtxrQK.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/0*S-zluU2G2ZG-l2mU.png"/></figure> <p>The math is beautifully simple: Data Scale = Number of Sensors × Time Elapsed</p> <p>This isn’t just about having more data-it’s about having fundamentally unlimited data collection capacity as every second elapses. The implications of this shift from scarce, human-created content to boundless, real-world capture are profound.</p> <h3>Beyond Human Bias</h3> <p>Here’s where it gets even more interesting: internet content, no matter how objective it tries to be, carries inherent human biases. Every author’s choice of words, every curator’s selection, every moderator’s decision-they’re all filtered through limited human perception and expression.</p> <p>Real-world capture, on the other hand, is fundamentally different. It records reality as it exists, bound by physics and social norms rather than human interpretation. While sensor distribution might create some bias, this is something we can systematically control and adjust-unlike the inherent biases in human-created content.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/0*4hh-CkuaPWen1VZF.png"/></figure> <h3>The Path to AGI: Unlimited Data, Unlimited Potential</h3> <p>We’re entering uncharted territory. With compute and budgets expanding, data has become the primary bottleneck in AI development. But what happens when that bottleneck disappears? When data becomes truly unlimited?</p> <p>Just as I once underestimated GPT-3’s capabilities despite my deep understanding of transformers and GPT-2, I suspect we’re underestimating the potential of unlimited real-world data. Could this be the key to achieving AGI-an AI that truly understands and interacts with the physical world?</p> <p>Just as GPT-3 surprised us with its capabilities, unlimited real-world data could enable breakthroughs in multiple domains — perhaps a robot that can adapt to any kitchen layout, or autonomous vehicles that can handle truly unpredictable scenarios.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ecxVwHntU8M2ryQ5.png"/><figcaption><em>Jensen Huang’s Presentation @ CES 2025</em></figcaption></figure> <p>The answer might lie in letting our compute loose on this unlimited stream of reality. The future of AI is in giving those algorithms a direct window into the real world.</p> <h3>Appendix</h3> <h3>Video Token Calculation</h3> <h4>Video Input Parameters</h4> <p>Resolution: 1080p (1920×1080 pixels)<br/>Frame rate: 30fps<br/>Duration: 32.8 seconds<br/>Color channels: RGB (3 channels)</p> <h4>Raw Data Calculation</h4> <p>Single frame pixels: 1920 × 1080 = 2,073,600 pixels<br/>Total frames: 32.8 seconds × 30fps = 984 frames<br/>Total raw pixels: 2,073,600 × 984 = 2,040,422,400 pixels</p> <p>Compression factors:<br/>8x temporal compression<br/>16x spatial compression (width)<br/>16x spatial compression (height)<br/>Total compression rate: 8 × 16 × 16 = 2048x</p> <h4>Final Token Count</h4> <p>Token calculation: 2,040,422,400 ÷ 2048 ≈ 996,300 ~ 1M tokens</p> <p><em>Originally published at </em><a href="https://huggingface.co/blog/kenhktsui/unlimited-training-data"><em>https://huggingface.co</em></a><em>.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=78f807a4f3b6" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Large Language Model 2023 Review and 2024 Outlook</title><link href="https://kenhktsui.github.io/blog/2024/large-language-model-2023-review-and-2024-outlook/" rel="alternate" type="text/html" title="Large Language Model 2023 Review and 2024 Outlook"/><published>2024-01-09T15:13:38+00:00</published><updated>2024-01-09T15:13:38+00:00</updated><id>https://kenhktsui.github.io/blog/2024/large-language-model-2023-review-and-2024-outlook</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2024/large-language-model-2023-review-and-2024-outlook/"><![CDATA[<p>TLDR: 2023 is <strong>The Year of Open Source LLM</strong>; 2024 will be <strong>The Year of SLM and Synthetic Data</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yFC890eaMALKrzqI8tVMQg.jpeg"/><figcaption>Mount Fuji by Author</figcaption></figure> <p>What a year in 2023! 2023 kickstarted with everyone including executives talking about ChatGPT at the beginning of the year. Given the realisation of imperfect yet unlimited potential (research and business) of LLM, the LLM space has drawn a lot of resources from elsewhere. LLM space has been growing exponentially, unlike the rest of the world.</p> <p>To explain why I feel LLM space is exponential, one anecdote is I did an hour sharing on LLM back in Feb 2023 focusing on the language model scaling law, supervised finetuning (SFT) and reinforcement learning with human feedback (RLHF), and when I looked back the slide now, it felt like they are no longer (so) relevant.</p> <p>As a open source contributor who had the opportunity to work with some brilliant independent researchers to build dataset/ train model; and also as a ML engineer who applies LLM in production and faces new production challenges, I have been with LLM more than ever.</p> <p>Same as <a href="https://medium.com/@kentsui/ai-machine-learning-2022-review-and-2023-outlook-a-practitioners-perspective-9d1b34c0a21f">last year</a>, I made an attempt to summarise what had come into my attention in 2023, and write an article in a day without the help of any LLM (You can tell from my writing) as I trust my first instinct without rethinking can bring me the most memorable moments in 2023.</p> <p>Despite reading &gt; 200+ paper, I might still miss quite a lot as an average person trying to catch an exponential trend, so feedbacks welcomed.</p> <h3>2023 Trends:</h3> <p><strong>1.Open-source LLMs</strong></p> <p>The release of <a href="https://arxiv.org/pdf/2307.09288.pdf">Llama 2</a> for research and commercial use is probably a defining moment in 2023 because in my opinion it not only started a culture of open sourcing high quality LLM without commercial limitation — the open culture that is quite deep rooted in AI research community, but also it has shown a belief to researchers that open source models can be at par with closed source models.</p> <p>Of course HuggingFace has always been a driving force behind to make sure any new model or new data is accessible.</p> <p><strong>2.Smaller Model Thanks to Quality Data</strong></p> <p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca-7B</a> demonstrated that with 52k high-quality alignment data and <a href="https://arxiv.org/pdf/2302.13971.pdf">LLama-7B</a> model, it is able to train a model that behaves similarly with GPT3 text-davinci-003.</p> <p><a href="https://arxiv.org/pdf/2305.11206.pdf">Less is More</a> showed that you do not need that much data to do alignment. 1,000 carefully curated prompts and responses are good enough.</p> <p><a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are All You Need</a> trained small models from scatch with textbook quality data, and synthetically generated textbook. <a href="https://huggingface.co/microsoft/phi-2">phi-2</a> shows SOTA performance among model ≤ 13 B.</p> <p><a href="https://arxiv.org/pdf/2306.02707.pdf">Orca</a> demonstrated that small language model can have better reasoning by imitating the reasoning process of larger LM.</p> <p>The drop of <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral 7B</a> was a big thing in the open source community because at the time of release, it was the best. <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral</a> is the first time that open source model can match and even outperform GPT-3.5, with a sparse mixture-of-expert network.</p> <p><strong>3.AI Feedback Instead of Human Feedback</strong></p> <p><a href="https://www.pnas.org/doi/10.1073/pnas.2305016120">ChatGPT outperforms crowd workers for text-annotation tasks</a> has shown that ChatGPT has demonstrated the potential of leveraging AI feedback without compromise of quality.</p> <p>It is not surprising to replace human feedback with AI feedback, once an aligned and good enough model exists as human feedback is expensive, and not scalable, and sometimes is wrong due to various factors like emotion, boredom, etc.</p> <p><a href="https://arxiv.org/pdf/2310.16944.pdf">Zephyr-7B</a> surpassed LLama2-Chat-70B. In particularly, it uses GPT-4 preference to rank the synthetic response data, which is later on used for <a href="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimisation</a>.</p> <p><strong>4. Synthetic Data</strong></p> <p>With better and better language model, synthetic data generated by LLM starts making sense.</p> <p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca-7B</a> leveraged language model to generate instruction data with <a href="https://arxiv.org/pdf/2212.10560.pdf">Self-Instruct</a>.</p> <p><a href="https://arxiv.org/pdf/2304.12244.pdf">WizardLM</a> introduced instruction evolution to turn an initial instruction into more complex instruction. It found that it outperforms ChatGPT when instruction is more complex.</p> <p><strong>5.Attempt to Lengthen Context Window</strong></p> <p>Long context window is important not only because it can cater more real life use case like understanding a long document, but also it enables model to solve problem that is very complex which requires long term reasoning.</p> <p><a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-Aware Scaled RoPE</a> introduced neural tangent theory (NTK) to interpolate RoPE Fourier space. Without finetuning, it’s able to achieve low perplexity in extended context window beyond training by only changing 3 lines of code</p> <p><a href="https://arxiv.org/pdf/2309.17453.pdf">StreamingLLM</a> allowed a model trained with limited context window to inference on infinite sequence length by leveraging the finding that keeping the hidden states (KV) of initial tokens can recover most performance.</p> <p><a href="https://arxiv.org/pdf/2307.03172.pdf">Lost in the Middle</a> showed that performance degrades in the middle of long context.</p> <p><a href="https://www.anthropic.com/index/claude-2-1-prompting">Long context prompting</a> increased long term recall significantly by adding “<em>Here is the most relevant sentence in the context:”. </em>Yet, this testing is not extended to other models because it is not a research paper.</p> <p><strong>6.Multimodel LLM</strong></p> <p>A lot of our daily use cases that needs visual cues can be empowered. It basically completes a personal assistant. GPT-4-V and open sourced <a href="https://arxiv.org/pdf/2304.08485.pdf">LLava</a> are great examples.</p> <p><strong>7.Prompt Engineering</strong></p> <p>There are quite a lot of research papers in prompt engineering.</p> <p><a href="https://arxiv.org/pdf/2305.10601.pdf">Tree of Thoughts</a> is the one that generalises Chain of Thoughts approach by allowing LLM to consider different reasoning paths towards the same problem. It can be further elevated to Monte Carlo tree search, which empowered <a href="https://www.nature.com/articles/nature16961">AlphaGo</a> to defeat human champion by 5 games to 0.</p> <p><a href="https://arxiv.org/pdf/2210.03629.pdf">ReAct prompting</a> is to add action on top of thoughts/ reasons so that a LLM does not limit itself to its own representation. It can interact with the external world for accurate and updated information.</p> <p><a href="https://arxiv.org/pdf/2302.04761.pdf">Toolformer</a> is an approach to explicitly teach language model to use tools like APIs or a calculator.</p> <p>With action added, it turns an LLM into an autonomous agent.</p> <p><strong>8.Quantised/ optimisation for training</strong></p> <p>This is a last but not least point.</p> <p>From float32 to bfloat16, <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">to 8-bit, to 4-bit</a>; from LoRa to <a href="https://arxiv.org/pdf/2305.14314.pdf">QLoRA</a>. It not just reduces GPU RAM requirement, but also training time. It empowers talented people without rich GPU access to perform proof of concept and research, so that the open source community continues to thrive.</p> <p>In short, rather than saying 2023 is The Year of LLM, I would say 2023 is The Year of <strong>Open Source</strong> LLM. LLM has become a commodity, and become accessible by people who are willing to learn. Having said that, using them in production still remains challenging because:</p> <ul><li>the no. of use case in this world is almost unlimited</li><li>100% correctness of LLM output still remains an open question</li><li>limited supply of high end GPU.</li></ul> <h3><strong>2024 Trends:</strong></h3> <p><strong>1.Small Language model (SLM, &lt;=2B)</strong></p> <p>I think deep learning has come to an era that everyone starts realising quality data is important, just like traditional machine learning. Quality data reduces conflicting learning objective, avoid local minimum and encourages faster convergence.</p> <p>The problems are:</p> <ul><li>what is “quality” data? Diverse and correct with sufficient reasoning and different complexities?</li><li>how to execute it at scale?</li></ul> <p>Out of the trillion tokens, how much is actually useful? If only 5% is useful, a much smaller model is possible to “compress” all training data and learn a meaningful representation. For example, <a href="https://arxiv.org/pdf/2306.11644.pdf">phi-1</a> model is only trained on 7B tokens, while <a href="https://arxiv.org/pdf/2307.09288.pdf">Llama2</a> is trained on 2T tokens.</p> <p>The secret recipe to a good LLM is the underlying data. As you can notice, everyone releases their model, but not everyone releases their training data.</p> <p><strong>2.Synthetic data</strong></p> <p>It is an inevitable route. There are 3 variables to a model (Dataset Size, Model Size, Compute) that determines a model’s power. Given the fixed amount compute, you can trade off between model size and data size. But what if you don’t have unlimited compute? You can always scale up a model, but not data.</p> <p>If the world does not contain enough high quality data, what can one do?</p> <p>Generate.</p> <p>If the world does not contain intelligent enough data to learn from to achieve superintelligence, what can one do?</p> <p>Generate.</p> <p>It makes sense to generate if you have a powerful LLM with verifiers.</p> <p><strong>3.Industry Specific Dataset &amp; Benchmark</strong></p> <p>The internet does not have all the data in this universe after all.</p> <p>Number of use case in this planet is a long tail distribution.</p> <p>Opportunity goes back to industry players who have a moat — private data that they could capitalise. There is no better time than now to create business value by digitalising physical document.</p> <p>One model to rule them all? Nah, centralised LLM will become decentralised and tailored to specific use cases.</p> <p><strong>4.LLM as a Search Engine</strong></p> <p>Before LLM, search engine can only search what already exists; now LLM can search for something that does not exist yet. It completely changes the universe of search space from limited to unlimited.</p> <p>Searching with LLM becomes meaningful if latency drastically decreases because one can simulate more given the same time. <a href="https://www.nature.com/articles/s41586-023-06924-6">FunSearch</a> searches for how to solve a problem by combining genetic algorithm and language model which is used as a solution generator and as an evaluator.</p> <p>Search becomes scalable with small language model, and quantitative change leads to qualitative change.</p> <p><strong>5.Reasoning Engine in Robotics</strong></p> <p>With smaller model, latency becomes low enough such that robot can act and react in real time, and it becomes possible to deploy LLM in edge device, so robot can run in an offline manner. Numerous researches shows that using text-only LLM as a reasoning engine can significantly improve robotics performance, not to mention leveraging multimodal LLM.</p> <p><strong>6.The Quest for AGI and Superintelligence Continues</strong></p> <p>It depends on the definition of AGI or superintelligence. Let’s define it as an intelligent agent who surpasses the intelligence of the most gifted human being.</p> <p>Crystalized from the past trends, there are few possible paths that will inevitably happen in my opinion:</p> <ul><li>Quantity : can 100 gifted minds imitated by language model surpass the most gifted human minds? It is not impossible if you view problem as considering all possible reasoning paths like Monte Carlo Tree Search and pick the weight adjusted choice.</li><li>Quality: can we generate data more intelligent than the original data that the generator was trained from? <a href="https://arxiv.org/pdf/2304.12244.pdf">WizardLM</a> is a good showcase that the difficulty of instruction be increased by LLM. LLM behaves like an approximate retrieval system because of its autoregressive nature so it might fail reasoning test if training data does not have so. Can we generate data that is not in training set of the generator model?</li></ul> <p>Quantity is possibly an easier path forward. After such system exists, the training signal can be distilled by another language model.</p> <p>In both cases, there exists a limit in the verification/ falsifiability of knowledge that that there is no LLM which can validate the generated solution. There is an exception through: a set of problems can be quickly checkable but not quickly solvable (in NP but not in P). They will become the new testbed for LLM.</p> <p>What about knowledge that is not quickly checkable? Maybe in some day LLM can prove P=NP or the vice versa.</p> <p>So, will superintelligence arrive at 2024? I don’t know, but the stage is set and it’s just a matter of time. For most people, it’s game changing enough when LLM can match the intelligence of an average person, and you can have multiple of them working for you.</p> <p><strong>What do you think? What’s waiting us in 2024?</strong></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cbd5211cf49b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Paper Digest: Think before you speak: Training Language Models With Pause Tokens</title><link href="https://kenhktsui.github.io/blog/2023/paper-digest-think-before-you-speak-training-language-models-with-pause-tokens/" rel="alternate" type="text/html" title="Paper Digest: Think before you speak: Training Language Models With Pause Tokens"/><published>2023-10-16T06:30:10+00:00</published><updated>2023-10-16T06:30:10+00:00</updated><id>https://kenhktsui.github.io/blog/2023/paper-digest-think-before-you-speak-training-language-models-with-pause-tokens</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2023/paper-digest-think-before-you-speak-training-language-models-with-pause-tokens/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IrlnTHVAWoMVTuk5KL-kjg.png"/></figure> <p><strong>Motivation:</strong></p> <p>Next token prediction imposes a constraint that the no. of operation predicting the next token is limited by the no. of token seen so far.</p> <p><strong>Hypothesis:</strong></p> <p>For some task, it demands more computation than there is, in next token prediction. By introducing &lt;pause&gt; token, it induces more computation, and therefor improve performance.</p> <p><strong>Reasoning Type:</strong></p> <p>Deductive</p> <p><strong>Reasoning Step:</strong></p> <p>To explain what it means, I drew the below to illustrate the effect of delaying next token prediction. The graph on the left is the standard next token prediction, the next token prediction goes over 2 transformer blocks to predict “jumps”.</p> <p>The graph on the right is after injecting the &lt;pause&gt; token. Due to the autoregressive nature of the model where the output in the previous step fed back as an input for the next step, the prediction goes through 4 transformers blocks before the model output “jumps”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/993/1*7JAEK3yAmxBZXxj99VARAA.png"/></figure> <p>As such, by introducing &lt;pause&gt; token, it induces more computation without changing model size.</p> <p><strong>Testing Approach:</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8j8qbDW_jjhuUnHMEaK0WQ.png"/><figcaption>Figure 1 from the Paper</figcaption></figure> <p>Two objectives were tested:</p> <ul><li>does delay token prediction improvements improvement?</li><li>when shall &lt;pause&gt; token be introduced, in pretraining or finetuning or both?</li></ul> <p>Decoder model of 1B and 130M was pretrained and finetuned, (probably due to computation resources because pretraining is required). Please also not that the &lt;pause&gt; token does not contribute into loss being optimised.</p> <p>Nine datasets were used.</p> <ul><li>GSM8k — high quality grade school maths</li><li>SQuAD V1 —question answering based on Wikipedia</li><li>CommonSenseQA — questions on common sense</li><li>LAMBADA — last word prediction on long context</li><li>WebQA — open Domain questions with multi-hop nature</li><li>NaturalQA — open Domain questions based on Wikipedia</li><li>CoQA — question answering based on context</li><li>PhysicalIQA — questions on physical common sense</li><li>HellaSwag — commonsense natural language inference</li></ul> <p><strong>Findings:</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Da91W17JLQp1BbVARfQ57Q.png"/></figure> <p>Introducing &lt;pause&gt; tokens in pretraining and finetuning outperforms standard pretraining and finetuning in most QA tasks except HellaSwag. Finetuning with &lt;pause&gt; tokens after standard pretraining has mixed result.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wr7rTDofKPvOzhhmunC-Hw.png"/></figure> <p>Adding filler token during inference time in standard pretrained and finetuned does not help performance.</p> <p>For each dataset, there exists an optimal no of &lt;pause&gt;.</p> <p><strong>My Take:</strong></p> <p>Despite limitations such as limited testing in bigger model sizes and benchmarking performance with larger model, I personally like this paper very much because it revisits the next token prediction objective, which is quite under-researched relatively in my opinion. The authors have opened the paradigm of delayed token prediction, and challenged the implicit assumption that the no. of operation predicting the next token is limited by the no. of token seen.</p> <p>Also, the author also laid out a very important open question: model parameter count vs computation pathways.</p> <p>In short, the idea is simple, but it challenged the existing assumption (next token prediction) and as such open the doors to many area to researchers.</p> <p>Inspired by the paper, I am extending my thoughts further:</p> <h4>Required Computation Complexity is Different For Each Token</h4> <p>Not every token prediction requires the same amount of computation.</p> <p>Sentence 1: [Description of Case] . The culprit ___</p> <p>Sentence 2:[Description of Case]. The culprit is ___</p> <p>For example, prediction in sentence 1 because it’s bounded by grammatical rule while prediction in sentence 2 requires very long reasoning (amount of computation) because guessing a culprit requires hypothesis, search, and elimination of hypothesis.</p> <p>Perhaps this is not very surprising. My <a href="https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231">previous paper review</a> highlights hallucination snowballing. One of the causes its authors attributed into is “Inherently sequential” - transformer cannot find an answer within one time step because of limiting reasoning ability in limited tokens. Therefore, a LM cannot answer a question requiring multiple steps of reasoning if a LM is guided to answer in one step.</p> <p>This is the nature of text. There is no correlation between the no of words and the amount of thought required to produce the text, because some thoughts were documented while most thoughts were not.</p> <p>This also explains why different dataset has an optimal no. of &lt;pause&gt; for the best performance. Some datasets are just harder than others, and requires more reasoning and computation.</p> <p>Future direction can be to leverage uncertainty of a language model prediction to construct a dataset that is filled with no of &lt;pause&gt; that corresponds to uncertainty.</p> <h4>Prompt Engineering As A Rescue?</h4> <p>Prompt engineering like <a href="https://arxiv.org/pdf/2201.11903.pdf">Chain of Thought</a>, few shot learning, or simply to append “Let’s think step by step” despite different intentions share one commonalities, which is to increase the amount of computation. For example, chain of thought prompts model to generate intermediate reasoning step before producing answer, as a rescue to provide more reasoning step. Few shot learning requires seeing a few demonstration before the prediction.</p> <p>Prompt engineering has a limitation that it can only be done in inference time, and sometimes there could be a mismatch between pretraining objective and inference setting. There can be an attempt to pretrain a model with a dataset with COT prompting, but it’s just so difficult to ensure the whole web text is filled with reasoning.</p> <h4>Soft Token vs Hard Token</h4> <p>Another question is if reasoning step has to be in language form? To a language model, token possesses much less information than vector. In chain of thought, one could always challenge why this reasoning step is used but not others? In fact, towards the same solution, there are many possible ways of reasoning. With language, we usually go with one way, but not all the ways. It is why language model succeeded because languaged is modeled as probabilities. And probabilities means there are different ways to express to same meaning.</p> <p>I have a conjecture that &lt;pause&gt; token serves as the soft version of chain of thought. Indeed soft token is not something new. Soft prompt was proved to boost performance just like hard prompt. <a href="https://arxiv.org/pdf/2104.08691.pdf">Prompt Tuning</a> is one of the examples, which prepends a series of tokens to condition model generation for specific tasks. The token is stored in embedding table.</p> <h4>Allocation of FLOP budget (Width vs Depth vs Model Size)</h4> <p>Computation pathway is an under discussed topic in the research field. There were research on how to achieve compute-optimal training by the famous <a href="https://arxiv.org/pdf/2203.15556.pdf">Chinchilla</a> paper, which answers the question of how to allocate model size and no. of training token given the same compute during training.</p> <p>With the model size and computation pathways being orthogonal, some open questions are:</p> <ul><li>What is the optimal balance of model parameter count vs computation pathways?</li><li>Can a 7B model perform similarly to a 175B model by increasing computation depth L to get roughly equal number of floating point operations?</li><li>How will this affect the design of transformers going forward? Can we reuses transformer block over time step, like parameter sharing across time step in RNN?</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*az1EAfFSFZsYbXTIk9KNzg.png"/><figcaption>Transformer Block 1 and 2 are reused.</figcaption></figure> <ul><li>Given the same parameter count, shall we prioritise less on no. of transformers block, while re-allocate more on dimension?</li></ul> <p>These are all interesting topics that I might conduct research and write a formal paper in the future if I have time.</p> <p><strong>About Paper Digest:</strong></p> <p>Paper Digest aims to digest a paper into a short summary while maintaining the essence of the scientific process behind the research. It serves as my personal reflection after understanding an academic paper.</p> <p>Previous articles:</p> <p><a href="https://medium.com/@kentsui/paper-digest-how-language-model-hallucinations-can-snowball-baaedd3d4231">Paper Digest: How Language Model Hallucinations Can Snowball</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=42be8c1ae785" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Paper Digest: How Language Model Hallucinations Can Snowball</title><link href="https://kenhktsui.github.io/blog/2023/paper-digest-how-language-model-hallucinations-can-snowball/" rel="alternate" type="text/html" title="Paper Digest: How Language Model Hallucinations Can Snowball"/><published>2023-07-16T04:49:56+00:00</published><updated>2023-07-16T04:49:56+00:00</updated><id>https://kenhktsui.github.io/blog/2023/paper-digest-how-language-model-hallucinations-can-snowball</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2023/paper-digest-how-language-model-hallucinations-can-snowball/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*-Yhm-B-yPXRnjeasJgK2Lw.png"/></figure> <p><strong>Motivation:</strong></p> <p>Is knowledge gap the only source of Language Model (LM) hallucination? Do LM only hallucinate when they do not know a fact?</p> <p><strong>Hypothesis:</strong></p> <p>LMs is vulnerable to hallucination snowballing — a phenomenon that LM tends to commit an answer in the first token, followed by an explanation coherent to the answer despite an incorrect answer.)</p> <p><strong>Reasoning Type:</strong></p> <p>Deductive</p> <p><strong>Reasoning Step:</strong></p> <p>“Initial commital”: The prompt often guides instruction-tuned LM to generate an answer before an explanation, because of how instruction data was designed. Even if the answer is wrong, the implicit coherence objective (via the next token prediction) requires the explanation to support a wrong answer so the explanation is also wrong.</p> <p>“Inherently sequential”: Transformer cannot find an answer within one time step because of limiting reasoning ability in limited tokens. Therefore, a LM cannot answer a question requiring multiple steps of reasoning if a LM is guided to answer in one step.</p> <p><strong>Testing Approach:</strong></p> <p>The authors prove the existence of hallucination snowballing — when model returns wrong answer, incorrect explanation was provided.</p> <p>3 datasets (namely Primality Test, Senator Search, Graph Connectivity) were designed to induce hallucination snowballing. LM is then used to check whether an explanation is correct or not in a new session.</p> <p>Properties of datasets:</p> <ul><li>yes/no question</li><li>not answerable in one time step</li></ul> <p><strong>Findings:</strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Q4y9FeQNNiWqTKW-9k4gDQ.png"/></figure> <p>ChatGPT and GPT-4 only gives correct yes/no answer 39.87% and 16.6% of the time while ChatGPT and GPT-4 can detect 67.37% and 87.03% of incorrect claim. It tells ChatGPT and GPT-4 knows they hallucinate but they still give wrong answer, showing the existence of hallucination snowballing</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g314vY3Jxw__mmAd0zNi5A.png"/></figure> <p>When appended the “Let’s think step by step” to the prompt, error rates were all greatly reduced.</p> <p><strong>My Take:</strong></p> <p>LM is as best as how we align it with human preference/ instruction. This paper reveals some limitations of the existing instruction dataset.</p> <p>Answer comes before an explanation, which is not native to the autoregressive nature. Daniel Kahneman’s “Thinking, Fast and Slow” defines System 1 and 2. An dataset with an answer immediately after a question elicits System 1 behaviour. More System 2 instruction dataset is required.</p> <p>Also, most instruction dataset focused on correctness and factuality that it does not involves scenario where LM is exposed to a wrong fact and correction has to be be made.</p> <p>It also hints that some modification of autoregressive model can be done. For example, can it place a different importance to its generation to avoid snowballing?</p> <p><strong>About Paper Digest:</strong></p> <p>Paper Digest aims to digest a paper into a short summary while maintaining the essence of the scientific process behind the research. It serves as my personal reflection after understanding an academic paper.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=baaedd3d4231" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">AI/Machine Learning 2022 Review and 2023 Outlook — A Practitioner’s Perspective</title><link href="https://kenhktsui.github.io/blog/2022/aimachine-learning-2022-review-and-2023-outlooka-practitioners-perspective/" rel="alternate" type="text/html" title="AI/Machine Learning 2022 Review and 2023 Outlook — A Practitioner’s Perspective"/><published>2022-12-31T04:37:14+00:00</published><updated>2022-12-31T04:37:14+00:00</updated><id>https://kenhktsui.github.io/blog/2022/aimachine-learning-2022-review-and-2023-outlooka-practitioners-perspective</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2022/aimachine-learning-2022-review-and-2023-outlooka-practitioners-perspective/"><![CDATA[<h3><strong>AI/Machine Learning 2022 Review and 2023 Outlook — A Practitioner’s Perspective</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*U3c5dUmEb1769yezHDLE-g.jpeg"/></figure> <p>In the last day of 2022, it’s always a great opportunity to summarise what had happened and to look ahead to how AI will further evolve. Of course, what I mention below is not exhaustive and is more of my personal opinion. I intend not to spend a lot time writing this article because it reflects the big picture I have in mind, and also a snapshot of my view.</p> <p>2022 has been an exciting year of AI, where we see some critical and influential achievements in AI, including but not limited to:</p> <ul><li>release of some mega size datasets like LAION-5B, that is critical to the success of Stable Diffusion</li><li>Stability AI releasing open-source Stable Diffusion models, which disrupts art &amp; design industry, and multiple variants of fine-tuned models were developed upon it</li><li>BLOOM, an open-source large language model whose development is boostrapped by HuggingFace, GENCI and IDRIS</li><li>launch of OpenAI’s ChatGPT, which leads to countless use case in multiple domains, from writing a joke/ a letter/ a contract/ code/ book to asking for technical/financial/ life advice</li></ul> <p>From the perspective of an applied machine learning researcher and practitioner, 2022 has shown that:</p> <p><strong>From Large Language Model to AGI</strong></p> <ul><li>we are much closer to AGI (artificial general intelligence) than we thought (well there is still dispute but we can see large language model could pass Turing test in multiple settings. Simply the definition of AGI, and its testing worth a long article)</li><li>way to AGI may be more about aligning large model behaviour with human behaviour as we see the difference betweenOpenAI’s GPT3.5 and ChatGPT</li><li>scaling law in language model continues to hold true</li><li>large language model no longer concerns about overfitting, as it memorises all knowledge possibly collected in the universe</li><li>transformer can handle very long sequences, with more GPUs</li></ul> <p><strong>Image Generation</strong></p> <ul><li>text is very important to control image generation, much better than predetermined classes</li><li>diffusion model becomes the de facto image generation architecture</li></ul> <p><strong>Open-source Collaboration</strong></p> <ul><li>open-source collaboration is a successful working model for AI</li></ul> <p>I am very excited about what will happen in 2023. Below are my predictions for 2023:</p> <p><strong>From Large Language Model to AGI</strong></p> <ul><li>larger language model (which does not need prediction, as we see GPT-4 is coming)</li><li>effort to understand why large language model is so good at zero shot learning/ look for smaller model size without performance degradation</li><li>continuous effort to align model behaviour with human behaviour via Reinforcement Learning</li><li>giving large model’s access to physical world</li><li>development of rigorous AI reasoning and causation test and dataset</li><li>more multimodal attempt</li></ul> <p><strong>AI Safety</strong></p> <ul><li>more advanced detection of unfair use of large model, as generative model becomes more advanced</li></ul> <p><strong>Open-source Collaboration</strong></p> <ul><li>development of open-source ChatGPT-like system (which is already happening)</li></ul> <p>As ML practitioners, given the rise of large language model, there are a lot of opportunities ahead:</p> <ul><li>have a productivity boost with ChatGPT-like system</li><li>do more creative and deeper work that can’t be found in ChatGPT-like system, better understanding of model behaviour, more rigorous scientific process and less on coding</li><li>produce specialised/ small version of ChatGPT-like system that can be deployed in private cloud</li><li>leverage ChatGPT-like system as a human-machine interface and a workflow orchestrator</li><li>building inputs for ChatGPT-like system</li><li>participate into open-source collaboration</li><li>be ready to finetune large model whose weight can’t be fit in 1 GPU</li></ul> <p>Again, all the above are my personal views and its not meant to be a scientific and complete review, as I do not spend more than two hours writing this.</p> <p>Last but not least, wish everyone a happy new year!</p> <p>Note 1 : Opinions expressed are solely my own and do not express the views or opinions of my employer.</p> <p>Note 2: The article is not generated by ChatGPT nor curated by it.</p> <p>Note 3: Machine learning and AI are used interchangeably in this article because the recent advancement in AI is mostly fuelled by machine learning.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9d1b34c0a21f" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">If you know SQL, you probably understand Transformer, BERT and GPT.</title><link href="https://kenhktsui.github.io/blog/2019/if-you-know-sql-you-probably-understand-transformer-bert-and-gpt/" rel="alternate" type="text/html" title="If you know SQL, you probably understand Transformer, BERT and GPT."/><published>2019-09-03T14:42:06+00:00</published><updated>2019-09-03T14:42:06+00:00</updated><id>https://kenhktsui.github.io/blog/2019/if-you-know-sql-you-probably-understand-transformer-bert-and-gpt</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2019/if-you-know-sql-you-probably-understand-transformer-bert-and-gpt/"><![CDATA[<h4>It’s all about query and retrieval.</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*55MzPXdGCTZBIVU82Y0TEw.jpeg"/><figcaption>Paris, France</figcaption></figure> <p>For those who have been following how Natural Language Processing evolved since 2017/2018, <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Transformer</a>/ <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a> is not a stranger to you. As there have been a lot of great detailed introductions written by <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Alexander Rush</a>, <a href="http://jalammar.github.io/illustrated-transformer/">Jay Alammar</a> and recently <a href="http://www.peterbloem.nl/blog/transformers">Peter Bloem</a>, I don’t intend to cover what’s under the hood; instead I am searching for the first principle and trying to make an analog with something that is easier to be understood — a database and memory.</p> <p>If you could understand SQL query, probably you can get the essence of the Transformer Architecture. Even if you are not, this should not be hard to understand — I am just extracting the author and title of all NLP papers from my “paperbase”.</p> <blockquote>Select Author, Title from Paperbase where Category = “NLP”</blockquote> <p>The query to database has to flexibly accommodate all sorts of keys (Category = “NLP”)and values (Author and Title). At the same time, the key and value have to flexibly serve whatever query a user is making. Behind SQL, it’s governed by a data schema the share across query, key and value. A valid record contains all items so that they could be used interchangeably.</p> <p><strong>Both are Queries in Nature</strong></p> <p>At the time the query is passed to the computer, it’s a two-step process:</p> <ul><li>First, the computer has to search and match your query to the key.</li><li>Second, the computer returns value of the same record of the key.</li></ul> <p>This is surprisingly very similar to Attention Mechanism, the backbone of the Transformer:</p> <ul><li>QKt is matching up the query and key. Mathematically speaking, it’s measuring the cosine similarity between Q and K. The higher the similarity the more relevant a record is.</li><li>Softmax(.)V is returning value of the associated records of the key. Mathematically speaking, softmax function will return a probability. This means, every record is probability-weighted.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/389/1*SqJjcVwwGacYcF6en3dVww.png"/><figcaption>Formula 1 in Transformer (Vaswani et al., 2017)</figcaption></figure> <p>There are two highlights I want to make:</p> <ul><li>The query-key matching, and data retrieval step jumps from IF/ELSE/AND/OR logic to probability based. This echos how classical/ rule base AI has evolved into probabilistic AI. After all it’s impossible to write down all the rules based on whatever conditions, the best policy is to encode so with probability.</li><li>The value being stored is a vector (an array of rational numbers), instead of an integer. There is much difference between an integer and rational numbers. After all, from 0 to 1 (2 integers), there are virtually infinite numbers inside (0.1,0.11,0.111,0.1111,…,etc.). This implies that if we associate each rational number with a record, potentially we can store a lot of records between 0 and 1.</li></ul> <p>I have drawn an analogy between SQL and Attention Mechanism.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/686ef18f421b07dfcce191735ceadd98/href">https://medium.com/media/686ef18f421b07dfcce191735ceadd98/href</a></iframe> <p><strong>Queries Enhance Understanding</strong></p> <p>Another good thing about attention mechanism is that we implicitly ask the machine to write the query, to use the returned result, and to integrate different returned result to form another meaningful query and result, etc.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/776/1*CisMJQcsBef8RpTHRp2pfQ.jpeg"/><figcaption>Queries enhance understanding</figcaption></figure> <p>At the beginning, if I don’t understand what CV is, I will query “CV” and all the computer vision papers will be returned so I know CV stands for computer vision. Reciprocally, if we don’t understand what a paper talks about, by querying the Title and returning Category, I know it’s a paper about NLP. Through iterative, if not exhaustive, query, we are understanding the records better via building association between records<strong>.</strong></p> <p>Likewise in Transformer, we are querying every record, so that each record’s (word’s) association across all other records (words) will be returned. For example, if I query “paper” probabilistically, it may give me back a probabilistic estimate with “read” being the most probable. Through exhaustive query of all words, the machine can develop understanding and relationship across all words.</p> <p>A Transformer has multiple heads of attention, and stacks attention over attention, and so you can imagine that Transformer is like groups of smart analysts who collaboratively uses advanced semantic SQL iteratively to dig out insight from a super large database; when multiple middle level managers receive the insight from their direct reports, they present the finding to their managers (tougher than dual reporting), who ultimately distill so before passing to the CEO.</p> <p><strong>From Transformer to BERT and GPT</strong></p> <p>The famous BERT and GPT are using the Transformer architecture to capture and store the semantic relationship among words from very large corpus. Basically, most other state-of-the-art language models are based on Transformer because this database-like structure allows better retrieval and therefore storage of more complicated relationship in bigger amount of data like language.</p> <p>And this database is transferable, you can think of any pre-trained model as compressed training data. Indeed, exploring these pre-trained models becomes almost the de facto exercise for most practitioners.</p> <p><strong>It’s All About Query and Retrieval</strong></p> <p>A good memory (aka database) is essential to intelligence. Every thought you have is actually making a query to your brain database which then returns relevant language, image and knowledge. It is not possible without effective query and retrieval. As a result, finding an efficient data structure for better query and retrieval is always the focus of AI research, and that’s why we got more and more novel deep learning architectures to play around!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7b197cb48d24" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/if-you-know-sql-you-probably-understand-transformer-bert-and-gpt-7b197cb48d24">If you know SQL, you probably understand Transformer, BERT and GPT.</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">How XLNet combines the best of GPT and BERT</title><link href="https://kenhktsui.github.io/blog/2019/how-xlnet-combines-the-best-of-gpt-and-bert/" rel="alternate" type="text/html" title="How XLNet combines the best of GPT and BERT"/><published>2019-06-22T18:21:16+00:00</published><updated>2019-06-22T18:21:16+00:00</updated><id>https://kenhktsui.github.io/blog/2019/how-xlnet-combines-the-best-of-gpt-and-bert</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2019/how-xlnet-combines-the-best-of-gpt-and-bert/"><![CDATA[<h4>Understanding the conceptual differences of GPT, BERT and XLNet in 3 min</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ogCHmKPP6ZP2TR8lFtli8Q.jpeg"/><figcaption>Centre Pompidou, Paris, France</figcaption></figure> <p>XLNet is a new pretrained model, which outperforms BERT consistently on 20 tasks, often by a large margin.</p> <p><strong>What?! And Why?</strong></p> <p>Without understanding machine learning, it’s not difficult to reckon that the more context we have captured, the more accurate the prediction can be made. As such, the ability of a model to capture the most context deeply and efficiently is the winning recipe.</p> <p>Let’s play a game — what are <strong>[Guess1]</strong> and <strong>[Guess2] </strong>in the below context?</p> <p>[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, <strong>[Guess1]</strong>,<strong> [Guess2]</strong>, ‘and’, ‘linguistics’]</p> <p>Given the 3 min constraint, let me reveal the answer, and instead I would ask you: which model (GPT, BERT, XLNet) you found the most helpful to find out the answer.</p> <p><strong>Answer:</strong> [‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, <strong>‘machine’</strong>,<strong> ‘learning’</strong>, ‘and’, ‘linguistics’]</p> <p>We are using the notation Pr(Guess|Context) onwards. Literally it means the probability of guess based on the context.</p> <p><strong>GPT — We read from left to right and so we do not know the context after ‘machine’, ‘learning’:</strong></p> <p>Pr(‘machine’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’])</p> <p>Pr(‘learning’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘machine’])</p> <p>Knowing ‘machine’ actually help you guess ‘learning’ because ‘learning’ frequently follows ‘machine’ as machine learning is popular.</p> <p><strong>BERT — We know both sides in contrast with GPT, but we are guessing ‘machine’, and ‘learning’ based on the same context:</strong></p> <p>Pr(‘machine’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘and’, ‘linguistics’])</p> <p>Pr(‘learning’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘and’, ‘linguistics’])</p> <p>Having ‘linguistics’ actually help you guess ‘machine’ ‘learning’ because you know that natural language processing is a beautiful blend of machine learning and linguistics. Even if you don’t know that, with the presence of ‘linguistics’, you at least know that the it is not ‘linguistics’.</p> <p>You can see the obvious cons of BERT is that it is not able to account for the fact that ‘machine’ and ‘learning’ make a quite common term.</p> <p>How do we combine both pros of GPT and BERT?</p> <p><strong>XLNet — The Best of Both:</strong></p> <p>Permutation! The power of permutation is that even if we only read from left to right, permutation allows us to capture the context of both sides (reading left to right, and reading right to left).</p> <p>One of the permutations that allows us to capture context of both sides: [‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘and’, ‘linguistics’, <strong>‘machine’</strong>,<strong> ‘learning’</strong>]</p> <p>Pr(‘machine’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘and’, ‘linguistics’])</p> <p>Pr(‘learning’|[‘Natural’, ‘language’, ‘processing’, ‘is’, ‘a’, ‘marriage’, ‘of’, ‘and’, ‘linguistics’, ‘machine’])</p> <p>This time, you have the full context, and you immediately can guess ‘learning’, after guessing ‘machine’. You can see clearly that XLNet combines the benefits of both GPT and BERT.</p> <p>That’s all, hopefully it’s just a 3 min read. Please clap and share if you enjoy this article! Of course, read the XLNet <a href="https://arxiv.org/pdf/1906.08237.pdf">paper</a> if you want to know more.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8aa917330ad1" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/understanding-the-difference-of-gpt-bert-and-xlnet-in-2-min-8aa917330ad1">How XLNet combines the best of GPT and BERT</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Do you Understand it?</title><link href="https://kenhktsui.github.io/blog/2019/do-you-understand-it/" rel="alternate" type="text/html" title="Do you Understand it?"/><published>2019-05-22T22:32:56+00:00</published><updated>2019-05-22T22:32:56+00:00</updated><id>https://kenhktsui.github.io/blog/2019/do-you-understand-it</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2019/do-you-understand-it/"><![CDATA[<p>An intro to Natural Language Processing</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*39gXq8nukxsj8D4x1d29Cw.jpeg"/></figure> <p><strong>Understanding a Language Started with Checking a Dictionary</strong></p> <p>I randomly browsed over the Oxford Dictionary of English today, with a quest to boil language down to the “first principle”.</p> <p>I first checked something that a kid will probably learn at age of 3 — the meaning of “good” — “To be desired or approved of”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/934/1*Wgf1BY3E5KZ6nGGXgdcGOA.png"/><figcaption>Screenshot from <a href="https://en.oxforddictionaries.com/">https://en.oxforddictionaries.com</a></figcaption></figure> <p>Pretending not knowing what “desire” means, I checked again — “Strong wish for or want something”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-6bpGEHuFlH65V-fb9FSFQ.png"/><figcaption>Screenshot from <a href="https://en.oxforddictionaries.com/">https://en.oxforddictionaries.com</a></figcaption></figure> <p>Again, what is “wish”? “Feel or express a strong desire”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/927/1*R0YwIPhk0-hK91tdq4qrJA.png"/><figcaption>Screenshot from <a href="https://en.oxforddictionaries.com/">https://en.oxforddictionaries.com</a></figcaption></figure> <p>Oh dear, it is a magical circular referencing. I am stuck and can only flip between “desire” and “wish” forever. From the dictionary, there is no way to truly understand the meaning of “desire” and “wish”.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/930/1*ecvwAULFlG-1BSTrL6xM_g.png"/></figure> <p>You might not see so immediately if you check harder word, because it can be explained by simpler word. The same for learning a foreign language. But if you check the simplest word, this circular or even self-referencing comes up. Taking one more example, the meaning of “meaning” reads “what is meant by a word, text, concept, or action.”</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/933/1*KGE7X1DafzYyrY5xx945eA.png"/><figcaption>Screenshot from <a href="https://en.oxforddictionaries.com/">https://en.oxforddictionaries.com</a></figcaption></figure> <p><strong>Meaning is Assigned by us</strong></p> <p>The only explanation is that meaning is assigned to words by us. We thought we understand them because everyone knows them, but actually we just memorize the meaning assignment but not because of how it spells (well, occasionally it is true as there are some word components that you knew). The spelling itself has no meaning in most cases.</p> <p>Once we assign a meaning to a word, we know meanings of others. It’s like a network of words. For example, if we understand “desire”, we know “wish” and “good”. Even if we don’t know the meaning of “desire”, we still know that “wish” and “good” have a similar context with “desire” if they occur in the same context.</p> <p>While the meaning of a word is assigned, we can guess so via observing the context. This is natural to human being in language comprehension.This links to the important concept — <a href="https://en.wikipedia.org/wiki/Distributional_semantics"><strong>Distributional Hypothesis</strong></a><strong> </strong>(words occur in the same context are more likely to have similar meaning), which had laid down the foundation of Natural Language Processing (NLP).</p> <p>This is the rough idea of word embedding in NLP. The fancy term “word embedding” is just that we are baking meaning into a series of numbers (vector) that a computer can understand and can use for downstream task such as sentiment analysis.</p> <p><strong>Machine can Learn how we Assign Meaning</strong></p> <p>Going back to the topic — Do we understand it? None of us, neither does computer. We don’t understand it, we assign meaning to it and we know “good” links to “desire” and “wish”. Computer can’t understand the spelling “good” neither, but it can learn our assignment distribution — “good” is similar to “desire” and “wish”. After all, the meaning assignment is still the property of human.</p> <p>P.S. Chinese might have a little bit different story because some evolves from pictograms.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b0b62657e836" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/do-you-understand-it-b0b62657e836">Do you Understand it?</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Why every data scientist shall read “The Book of Why” by Judea Pearl</title><link href="https://kenhktsui.github.io/blog/2018/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl/" rel="alternate" type="text/html" title="Why every data scientist shall read “The Book of Why” by Judea Pearl"/><published>2018-12-20T13:12:54+00:00</published><updated>2018-12-20T13:12:54+00:00</updated><id>https://kenhktsui.github.io/blog/2018/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2018/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl/"><![CDATA[<h4>How causality can further advance data science and AI</h4> <p>I have been a big fan of machine learning for 4 years and deep learning for more than a year. I built predictive model for fun and work. I know a lot of algorithms, from the conventional one like gradient boosting to the deepest model like LSTM. Despite numerous algorithms I had acquired, my puzzle remains.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Mx2TlL5863ue-ZceLiWlWg.jpeg"/></figure> <p><strong>Puzzle That Algorithm Itself Cannot Solve</strong></p> <p>If you are not the kind of data scientist who only cares how to reduce that 0.01% error but try to make sense of your model, you might have questioned yourselves from time to time:</p> <ol><li>Should I add this variable to my model?</li><li>Why does this counter-intuitive variable show up as a predictive one?</li><li>Why does this variable suddenly become insignificant if I add another variable?</li><li>Why does the direction of the correlation being opposite to what you think?</li><li>Why is the correlation zero when I thought it would be higher?</li><li>Why does the direction of relationship reverse when I dis-aggregate the data into sub-population?</li></ol> <p>Over the time, I have built up enough sense to tackle these fundamental questions, for example, I know bi-variate relationship can be very different from multi-variate relationship or the data is subject to selection bias. But it lacks a solid framework that I could convince myself definitely and others. More importantly, I might not be aware until a relationship contradicts my sense! Important to note, when something contradicts, it has already gone very wrong. Without a map, how can I be sure I am not heading to the wrong destination before I know I am lost?</p> <p><strong>Yes, Both Association and Causality Can Predict</strong></p> <p>The puzzle has completely gone when I read “The Book of Why” by Judea Pearl. Now it is my guide for data science. Here I will tell you WHY briefly. In short, it is causality, the relationship between cause and effect. To predict something in the future, there are two ways:</p> <ol><li>I know that when I see X, I will see Y (Association)</li><li>I know that X causes Y (Causality)</li></ol> <p>Both ways can predict. Both ways might yield similar model performance. So, what is the difference? Why bother to understand causality? And if it is a more powerful tool, can causality be studied via data?</p> <p><strong>Random Controlled Trial and Why It Is Not Practicable Sometimes</strong></p> <p>As a gold standard, randomized controlled trail (RCT) (or so called A/B test in marketing) is used to test for causality. Particularly, in clinical trial, this technique is used to study whether a particular medicine/ treatment can improve health.</p> <p>Randomization is to <strong>minimize selection bias</strong>, so that we know, for example, we do not selectively apply treatment to more ill patient, resulting in an apparently lower benefit that it wouldn’t be if we did not select more sick patient. Control is to act as a benchmark so that we can compare patient that did receive treatment vs patient who did not receive a treatment. As a standard, there is also a so called double blind mechanism, so that the patients did not know whether they actually receive treatment or not, in order to screen off psychological effect.</p> <p>While it is a gold standard, it might not be practicable in some circumstances. For example, if we want to study the effect of smoking on lung cancer, apparently we can’t force someone to smoke. Another example: if I want to know how far I would go if I did a PhD, definitely it is not possible as time will only go forward. After all, an experiment is subject to a lot of limitations, for example, whether the sample can represent the whole population, whether it’s ethical, etc.</p> <p><strong>From Observational Data to Causality Analysis?</strong></p> <p>If it is not practical to conduct an experiment, can we use the observational data to study causality? The observational data means we can’t do any intervention, we can just observe. Is that possible?</p> <p>No matter you know statistics or not, you might have heard the saying that correlation does not imply causation. However, it does not tell you how to study causality. The good news here is ,after reading this book, you will have a better framework of how to study causality, and determine when you can or cannot study it given the data you have and therefore you know what data you should collect.</p> <p><strong>Takeaways From The Book</strong></p> <p>I am not covering the detailed techniques or formula here. On one hand, I just finished reading this book and I am no expert of causality; on the other hand, I encourage you to read the book in order not to miss any insight because I might have bias.</p> <p><strong>Despite the prominence of big data, it might be wrong to add everything into your model</strong></p> <p>In the era of big data, with virtually unlimited computational power and data, you might be tempted to put every data into a deep neural network for auto feature extraction. I was tempted too.</p> <p>This book tells you a few scenarios that adding certain variable needs caution. For example, you want to predict Z, and the underlying relationship is X →Y → Z (arrow denoting “causes” and in this case Y is a mediator, which mediates effect from X to Z). If you add X and Y as your variables in your model, Y might absorb all the “explanatory power”, which kicks X out of your model because Y is more direct than X from the perspective of Z. This prevents you from studying the causality between X to Z. You might argue, there is no difference in prediction, isn’t it? Yes, from the viewpoint of model performance, but what if I tell you Y is so close to Z such that by the time you know Y, Z already occurred.</p> <p>Likewise, not adding certain variable is risky. You might have heard of the term spurious correlation or confounding. The basic idea can be illustrated in this relationship, Z← X → Y (i.e. X is a confounder). Note that there is no causal relationship between Z and Y, but if you don’t consider X, there appears to be a relationship between Z and Y. A famous example is the positive correlation between chocolate consumption and no. of Nobel Prize winner. It turns out that the common cause is the wealthy of a country. Again, you might have no problem of prediction, but you probably have a hard time to explain your model to others.</p> <p>Of course, the world is more complicated than we thought. But this is where domain knowledge plays in and causal diagram is a simple yet powerful representation of how everything works.</p> <p>There are more advanced brain teasers and real life examples in the book. Fortunately, the rules provided make them easy to follow.</p> <p><strong>Causality might be more robust</strong></p> <p>Causality might change over time. If you want your model to be robust over time. Taking Z← X → Y as an example, if the relationship Z← X has weakened, it wouldn’t impact you if you model X → Y, but it would if you model Z and Y.</p> <p>From another perspective if we believe causality is a stronger relationship than association, it means that relationship is more likely to hold if we borrow it from one area to another. This is so called transfer learning/ transportability, as referred in the book. The book quotes a very insightful example of transportability, and it describes how we can transparently perform adjustment so that we can transport the causal relationship from one area to another.</p> <p><strong>Intervention becomes much easier, particularly in the digital age</strong></p> <p>Intervention is actually one of the great motivations of studying causality. Predictive model by learning association alone cannot give you insight to intervene. For example, you can’t change Z to impact Y in Z← X → Y, because there is no causal relationship.</p> <p>Intervention itself is a much more powerful tool as you can understand the underlying relationship. This means, you can change government policy to make our world a better place; you can change treatment to save more patients, etc.. This is difference in you saving the patient vs you predicting the patient will die but can’t intervene! Perhaps this is the best thing a data scientist can do, only with this toolkit!</p> <p>In this digital age, intervention takes less effort, and for sure, you have more data to study causality.</p> <p><strong>It is how we reason and as such it might be the route to real AI</strong></p> <p>Finally, it is about artificial intelligence. Reasoning is a necessary part of intelligence and it is how we feel. In a closed world, with well defined reward and rules, reinforcement learning achieves excellence via a balance of exploring and exploitation to maximize pre-defined reward under the pre-defined rule, and under the mechanism that the action taken changes the state which in turn determines the reward. In this complicated world, this mechanism is less likely to hold.</p> <p>From a philosophical perspective, we should understand how we make a decision. Most likely, you will ask, “If I do this, what would happen; if I do that, then?”. Note, you just created two imaginary worlds that has not happened. Sometimes, when you do some reflection to learn from mistake, you might ask, “If I had done this, that would not happen.” Again, you created your counterfactual world. Indeed we are more imaginative than we thought. And these imaginative worlds are built on causality.</p> <p>Perhaps robot might have their own logic, but if we want it to be like us, we need to teach them reasoning. It reminds me of a paper published by DeepMind, “Measuring abstract reasoning in neural networks” (<a href="https://deepmind.com/blog/measuring-abstract-reasoning/">link</a>), which shows adding the reason as part of the training data can enhance generalization. I was deeply inspired by this paper, this is exactly the case where we teach robots reasoning! And it is a leap from association in pattern to reasoning.</p> <p>I conjecture that <strong>causality helps generalization</strong>. I don’t have a proof, but this is how we makes sense of the world. We are taught with one or two examples, then we learn the casual relationship, and we apply this relationship wherever we think it is applicable.</p> <p>Putting everything into a casual diagram, perhaps reasoning is the confounder of question and answer of IQ test? Can we argue reasoning causes one to design those pattern in the question, and it also “causes” the answer? Alternatively, it can be a mediator that translates the question into reasoning, which in turn causes the answer? Or it could be both? Note that I deliberately assume no casual relationship between Question and Answer as they are purely pattern association.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/663/1*JgHFX8omdJew0crePNxpCw.jpeg"/><figcaption>Reasoning as a Confounder</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/701/1*XMw2c-iNbzfRO3xATMifEw.jpeg"/><figcaption>Reasoning as a Mediator</figcaption></figure> <p>They are just my wild guesses. I don’t know the answer. I am not a full time researcher nor a philosopher. But what I am sure is that causality provides a new perspective when we attack a problem. The synergy between causality and deep learning sounds promising.</p> <p><strong>Final Thought</strong></p> <p>I admit that the topic of this article might be a bit aggressive, but I feel duty-bound to recommend this book to everyone. It tells us the full potential of causality, something inborn with us but we might overlook in the era of big data. The framework, do-calculus, is already there. It just waits for us to deploy and put it into practice.</p> <p>As a practitioner, with this powerful tool, I trust we can make a better impact.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2dad84b3f9d" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/data-science/why-every-data-scientist-shall-read-the-book-of-why-by-judea-pearl-e2dad84b3f9d">Why every data scientist shall read “The Book of Why” by Judea Pearl</a> was originally published in <a href="https://medium.com/data-science">TDS Archive</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Balance – It’s how everything behaves except most of us.</title><link href="https://kenhktsui.github.io/blog/2018/balance-its-how-everything-behaves-except-most-of-us/" rel="alternate" type="text/html" title="Balance – It’s how everything behaves except most of us."/><published>2018-10-28T17:01:58+00:00</published><updated>2018-10-28T17:01:58+00:00</updated><id>https://kenhktsui.github.io/blog/2018/balance--its-how-everything-behaves-except-most-of-us</id><content type="html" xml:base="https://kenhktsui.github.io/blog/2018/balance-its-how-everything-behaves-except-most-of-us/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/621/1*1bgt7gU-t8h2Oa5bVkXKsw@2x.jpeg"/></figure> <p>Earth exists because of balance of gravitational forces of other planets and stars; and the balance of gravitation force itself and the geothermal energy. It may be coincident but it can’t be randomly formed.</p> <p>Weather is the balance of day and night, rain and drought, high and low, which forms fountain and sea to nurture lives.</p> <p>Our body is about regulation of energy and chemical and therefore a lot of biological systems have negative feedback mechanism to balance all sort of things.</p> <p>Law is the balance of defining too much and defining too little. Defining too much will lose the generality, defining too little will open loophole.</p> <p>Writing or composing is about the balance of structures, particularly in poem or classical music. And so is the story flow, which need to have a right balance of prelude and climax.</p> <p>Microeconomics is about the balance of demand and supply with the help of price adjustment; and macroeconomics is about the balance of economic growth, inflation, interest rate, etc.</p> <p>Stock market and property price can’t go up forever, it has to plummet eventually if it is not in its equilibrium. And it has to rebound if it is too low. It’s ultimately subject to balance.</p> <p>Machine learning is about the balance of overfitting and under-fitting, and about the balance of exploration and exploitation.</p> <p><strong>Balance is the key to achieve stability which can stand against the test of time and disruption. There is nothing which could stand still without balance in itself. It does not work in isolation but seamlessly integrates in a larger ecosystem as if it was invisible.</strong></p> <p>Before we learned to walk, we had learned balancing ourselves. It is the idea we had learned long time ago and practised physically everyday but might have forgotten it mentally.</p> <p>Have we balanced ourselves? Are we too busy earning a living without knowing our conscience? <strong>We were born with no desire, except the biological needs. If our desire is a disrupting force to our balance, what is then the rebalancing force?</strong></p> <p>It might be a state of mind that we desire nothing. <strong>We are taught to get, perform and achieve, but we are not taught to stop and get nothing</strong>. When we desire nothing, we will not favour one to another; we will not be influenced by emotion; we are like a new-born baby who has no concept of knowledge but can rewire our brain to absorb knowledge unconsciously like a sponge. It is a perspective that we might have never explored.</p> <p><strong>It is not pessimistic. It’s just how every single thing behaves. Everything stays in its balance without leaving it and together forms an eco-system that we can’t notice. It exists in the past, the present and the future. It’s one the principles behind the universe we should learn and perhaps imitate.</strong></p> <p>Meditate and be selfless, give yourselves 10min to observe your balance and imbalance.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/621/1*htzkm_sDeVflOIly34elRQ@2x.jpeg"/></figure> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aa3ad0ab8a26" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>